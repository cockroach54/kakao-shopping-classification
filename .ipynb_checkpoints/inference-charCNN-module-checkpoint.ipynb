{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4215\n"
     ]
    }
   ],
   "source": [
    "import _pickle\n",
    "y_vocab = _pickle.load(open('./data_org/y_vocab.py3.cPickle', 'rb'))\n",
    "y_vocab['43>109>1576>-1']\n",
    "print(len(y_vocab))\n",
    "\n",
    "token_to_cate = {}\n",
    "for it in y_vocab.items():\n",
    "     token_to_cate[it[1]] = it[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_vocab = pd.read_csv('data_org/char.csv', encoding='utf8')\n",
    "x_vocab = x_vocab.as_matrix()\n",
    "#  3000개 = 2999 + unk\n",
    "x_vocab = {\n",
    "    e[0]:i  for i,e in enumerate(x_vocab[:2998])\n",
    "}\n",
    "x_vocab['<PAD>'] = 2998\n",
    "x_vocab['<UNK>'] = 2999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import h5py\n",
    "import mmh3\n",
    "\n",
    "seq_len=128\n",
    "random.seed(2018)\n",
    "\n",
    "def set_test_data(chunk_no):\n",
    "    path_x = 'data_org/dev.chunk.0%d'%(chunk_no)\n",
    "    h = h5py.File(path_x, 'r')\n",
    "    \n",
    "    mode = 'dev'\n",
    "    cols = ['pid', 'product', 'model', 'brand', 'maker', 'price', 'updttm', 'bcateid', 'mcateid', 'scateid', 'dcateid']\n",
    "    data = {\n",
    "        c: h[mode][c][: 1000000] for c in cols\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    for i in ['pid', 'product', 'model', 'brand', 'maker', 'updttm']:\n",
    "        df[i] = df[i].apply(lambda x: x.decode('utf8'))\n",
    "\n",
    "    df2 = pd.DataFrame(h[mode]['img_feat'][:1000000])\n",
    "    return df, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make DAG\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(2018)\n",
    "\n",
    "# train Parameters\n",
    "output_dim = 4215 # 1번 트레이닝셋만\n",
    "\n",
    "epoch = 100\n",
    "# seq_len=100\n",
    "vocabulary_size = len(x_vocab) # x_vocab length\n",
    "# embedding_size = 512\n",
    "embedding_size = 256\n",
    "features = 200\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "is_training = tf.placeholder(tf.bool) # 배치놈 위한 트레이닝/테스트 구분 불리언\n",
    "\n",
    "Y = tf.placeholder(tf.int16, [None, output_dim], name=\"label\")\n",
    "lr = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "keep_prob = tf.placeholder(tf.float32, [], name=\"keep_prob\"\n",
    "                          )\n",
    "\n",
    "#  embedding\n",
    "X_fw = tf.placeholder(tf.int32, [None, seq_len], name=\"word_tokens_fw\") \n",
    "X_bw = tf.placeholder(tf.int32, [None, seq_len], name=\"word_tokens_bw\") \n",
    "word_embeddings = tf.get_variable(\"word_embeddings\",\n",
    "    [vocabulary_size, embedding_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "embedded_fw = tf.nn.embedding_lookup(word_embeddings, X_fw) # batch * seq * embeddding\n",
    "embedded_bw = tf.nn.embedding_lookup(word_embeddings, X_bw) # batch * seq * embeddding\n",
    "    \n",
    "# image features\n",
    "Xm = tf.placeholder(tf.float32, [None, 2048], name=\"img_feat\") \n",
    "\n",
    "# price feature\n",
    "Xp = tf.placeholder(tf.float32, [None, 2], name=\"price\") \n",
    "\n",
    "\"\"\"\n",
    "#  dropout layer\n",
    "def _sequence_dropout(step_inputs, keep_prob):\n",
    "        # apply dropout to each input\n",
    "        # input : a list of input tensor which shape is [None, input_dim]\n",
    "        with tf.name_scope('sequence_dropout') as scope:\n",
    "            step_outputs = []\n",
    "            for t, _input in enumerate(step_inputs):\n",
    "                step_outputs.append( tf.nn.dropout(_input, keep_prob) )\n",
    "        return step_outputs\n",
    "\n",
    "embedded_fw = tf.unstack(embedded_fw, axis=1)\n",
    "step_inputs = _sequence_dropout(embedded_fw, keep_prob) # seq * batch * embedding\n",
    "\n",
    "#  FCN layer\n",
    "doc_mean = tf.reduce_mean(step_inputs, axis=0) # batch * embedding (mean)\n",
    "# hint = tf.placeholder(tf.float32, [None, 609], name='hint') # previous category\n",
    "# bf_lenear = tf.concat([doc_mean, hint], axis=1) # batch * (embediing + hint)\n",
    "# bf_lenear = tf.nn.dropout(bf_lenear, keep_prob)\n",
    "\"\"\"\n",
    "\n",
    "embedded_fw = tf.reshape(embedded_fw, [-1, seq_len, embedding_size, 1])\n",
    "embedded_bw = tf.reshape(embedded_bw, [-1, seq_len, embedding_size, 1])\n",
    "# CNN layer\n",
    "def char_cnn(no, embedded, name):\n",
    "  F = tf.get_variable(name, [no, embedding_size, 1, features], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer()) # (row, column, channel, kernels)\n",
    "  L = tf.nn.conv2d(embedded, F, strides=[1, 1, 1, 1], padding='VALID')\n",
    "  L = tf.layers.batch_normalization(L, training=is_training)\n",
    "  L = tf.nn.tanh(L)\n",
    "  return L\n",
    "\n",
    "C2f = char_cnn(2, embedded_fw, name='2gram-filter-fw') # batch, seq-1, 1, filters\n",
    "C2fm = tf.nn.max_pool(C2f, ksize=[1, seq_len-(2-1), 1, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "C2b = char_cnn(2, embedded_bw, name='2gram-filter-bw')\n",
    "\n",
    "C3f = char_cnn(3, embedded_fw, name='3gram-filter-fw')\n",
    "C3fm = tf.nn.max_pool(C3f, ksize=[1, seq_len-(3-1), 1, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "C3b = char_cnn(3, embedded_bw, name='3gram-filter-bw')\n",
    "\n",
    "C4f = char_cnn(4, embedded_fw, name='4gram-filter-fw')\n",
    "C4fm = tf.nn.max_pool(C4f, ksize=[1, seq_len-(4-1), 1, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "C4b = char_cnn(4, embedded_bw, name='4gram-filter-bw')\n",
    "\n",
    "# C5f = char_cnn(5, embedded_fw, name='5gram-filter-fw')\n",
    "# C5fm = tf.nn.max_pool(C5f, ksize=[1, seq_len-(5-1), 1, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "# C5b = char_cnn(5, embedded_bw, name='5gram-filter-bw')\n",
    "\n",
    "# 1*1 conv\n",
    "def one_cnn(fw, bw, name):\n",
    "    Concat = tf.concat([fw, bw], 3)\n",
    "    F = tf.get_variable(name, [1, 1, 2*features, 1], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer()) # (row, column, channel, kernels)\n",
    "    L = tf.nn.conv2d(Concat, F, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    L = tf.layers.batch_normalization(L, training=is_training)\n",
    "    L = tf.nn.tanh(L)\n",
    "    return L\n",
    "\n",
    "C2all = one_cnn(C2f, C2b, '2gram-1by1-filter')\n",
    "C3all = one_cnn(C3f, C3b, '3gram-1by1-filter')\n",
    "C4all = one_cnn(C4f, C4b, '4gram-1by1-filter')\n",
    "C_all = tf.concat([C2all, C3all, C4all], 1)\n",
    "C_all = tf.squeeze(C_all, axis=[2,3]) # (?, seq_len*3 - some)\n",
    "\n",
    "conv_list = [C2fm, C3fm, C4fm]\n",
    "C_cat = tf.concat(conv_list, 3) # (?, 1, 1, features*3) --> to attention\n",
    "C_flat = tf.reshape(C_cat, [-1, len(conv_list)*features]) # (?, features*3)\n",
    "C_flat = tf.contrib.layers.fully_connected(C_flat, len(conv_list)*50, activation_fn=tf.nn.elu) #(?, 50*3)\n",
    "\n",
    "# 이미지\n",
    "# Xm = tf.nn.dropout(Xm, keep_prob)\n",
    "Xm2 = tf.contrib.layers.fully_connected(Xm, features, activation_fn=tf.nn.elu) # (?, features*3)\n",
    "\n",
    "# dot product attention\n",
    "Xm2_3d = tf.reshape(Xm2, [-1, features, 1])\n",
    "C_cat_3d = tf.reshape(C_cat, [-1, len(conv_list), features])\n",
    "A = tf.matmul(C_cat_3d, Xm2_3d) # ?, 3, 1\n",
    "A_soft = tf.nn.softmax(A, axis=1)\n",
    "ww = A_soft*C_cat_3d # ?, 3, 200\n",
    "att = tf.reduce_sum(ww, axis=1) # ?, 200\n",
    "\n",
    "C_flat_final = tf.concat([C_flat, C_all, att, Xm2, Xp], axis=1)\n",
    "\n",
    "# 오캄의 면도날... 굳이 없어도 되는 레이어인듯\n",
    "# L_linear = tf.contrib.layers.fully_connected(C_flat_final, 512, activation_fn=tf.nn.relu)\n",
    "# L_linear = tf.nn.dropout(L_linear, keep_prob)\n",
    "L_linear = tf.nn.dropout(C_flat_final, keep_prob)\n",
    "\n",
    "\n",
    "# Y_pred = tf.contrib.layers.fully_connected(bf_lenear, output_dim, activation_fn=tf.nn.relu)  # We use the last cell's output\n",
    "L_linear2 = tf.contrib.layers.fully_connected(L_linear, 1024, activation_fn=tf.nn.elu)  # We use the last cell's output\n",
    "Y_pred = tf.contrib.layers.fully_connected(L_linear2, output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "\n",
    "# # image feature\n",
    "# X = tf.placeholder(tf.float32, [None, len(data_x[0])], name=\"img_feat\")\n",
    "# X = tf.nn.dropout(X, keep_prob)\n",
    "# Y_pred = tf.contrib.layers.fully_connected(X, output_dim,\n",
    "#                                            activation_fn=tf.nn.relu, weights_initializer=tf.contrib.layers.xavier_initializer())  # We use the last cell's output\n",
    "\n",
    "# for batch norm\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "# optimize\n",
    "cost =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=Y_pred, labels=Y, name='cross_entropy'))\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = optimizer.minimize(cost)\n",
    "\n",
    "# prediction\n",
    "predicted = tf.argmax(Y_pred, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, tf.argmax(Y, 1)), dtype=tf.float32))\n",
    "\n",
    "# saver\n",
    "name_to_var_map = {var.op.name: var for var in tf.global_variables()}\n",
    "saver = tf.train.Saver(name_to_var_map, name='my_saver', max_to_keep=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./small_model/small-71369\n"
     ]
    }
   ],
   "source": [
    "# restore model\n",
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "ckpt_path = './small_model/small'\n",
    "saver.restore(sess, ckpt_path+'-71369')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make .tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab3fedeb9f64f8590d4ebfbb8ccb473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in log10\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in log10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-97b34719bcb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         dev = sess.run([predicted], feed_dict={X:data_x[chunk_size*cnt:chunk_size*(cnt+1)], Xm:data_xm[chunk_size*cnt:chunk_size*(cnt+1)],\n\u001b[0m\u001b[0;32m     34\u001b[0m                                                Xp:data_xp[chunk_size*cnt:chunk_size*(cnt+1)], keep_prob:1, is_training:False})\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "chunk_size = 5000\n",
    "cate_predicted = []\n",
    "pid = []\n",
    "\n",
    "for no in [1]:\n",
    "    # set test data file\n",
    "    df, df2 = set_test_data(no)\n",
    "\n",
    "    # df to data - char to token\n",
    "    # df to data - char to token\n",
    "    data_x_fw = []\n",
    "    data_x_bw = []\n",
    "    for i in tqdm(list(zip(df_val['product'], df_val['brand'], df_val['model'], df_val['maker']))):\n",
    "        sentence = ' '.join(i)\n",
    "        sentence = list(sentence)    \n",
    "        # hash --> word to id\n",
    "        word_ids_fw = [x_vocab[k] if k in x_vocab.keys() else 2999 for k in sentence][:seq_len]\n",
    "        word_ids_fw = np.pad(word_ids_fw, (0,seq_len-len(word_ids_fw)), 'constant', constant_values=(2998)) # pad=2998\n",
    "        data_x_fw.append(word_ids_fw)\n",
    "        # revcerse\n",
    "        sentence.reverse()\n",
    "        word_ids_bw = [x_vocab[k] if k in x_vocab.keys() else 2999 for k in sentence][:seq_len]\n",
    "        word_ids_bw = np.pad(word_ids_bw, (0,seq_len-len(word_ids_bw)), 'constant', constant_values=(2998)) # pad=2998\n",
    "        data_x_bw.append(word_ids_bw)\n",
    "\n",
    "    X_fw_val = np.array(data_x_fw)[:2000]\n",
    "    X_bw_val = np.array(data_x_bw)[:2000] \n",
    "    \n",
    "    iter_num = ceil(len(data_x)/chunk_size)\n",
    "    pid += list(df['pid'])\n",
    "    \n",
    "    for cnt in range(iter_num):\n",
    "        dev = sess.run([predicted], feed_dict={X:data_x[chunk_size*cnt:chunk_size*(cnt+1)], Xm:data_xm[chunk_size*cnt:chunk_size*(cnt+1)],\n",
    "                                               Xp:data_xp[chunk_size*cnt:chunk_size*(cnt+1)], keep_prob:1, is_training:False})\n",
    "        for i in dev[0]:\n",
    "            tmp = token_to_cate[i]\n",
    "            cate_predicted.append(tmp.replace('>', '\\t'))\n",
    "\n",
    "assert len(cate_predicted) == len(pid)\n",
    "with open(\"final.predict.tsv\", \"w\") as f:\n",
    "    for el in zip(pid, cate_predicted):\n",
    "#         print(el)\n",
    "        tmp = '\\t'.join(el)      \n",
    "        f.write(tmp+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
